{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "\n",
    "def readGz(f):\n",
    "  for l in gzip.open(f):\n",
    "    yield eval(l)\n",
    "\n",
    "### Rating baseline: compute averages for each user, or return the global average if we've never seen the user before\n",
    "\n",
    "allRatings = []\n",
    "userRatings = defaultdict(list)\n",
    "for l in readGz(\"train.json.gz\"):\n",
    "  user,business = l['userID'],l['businessID']\n",
    "  allRatings.append(l['rating'])\n",
    "  userRatings[user].append(l['rating'])\n",
    "\n",
    "globalAverage = sum(allRatings) / len(allRatings)\n",
    "userAverage = {}\n",
    "for u in userRatings:\n",
    "  userAverage[u] = sum(userRatings[u]) / len(userRatings[u])\n",
    "\n",
    "predictions = open(\"predictions_Rating.txt\", 'w')\n",
    "for l in open(\"pairs_Rating.txt\"):\n",
    "  if l.startswith(\"userID\"):\n",
    "    #header\n",
    "    predictions.write(l)\n",
    "    continue\n",
    "  u,i = l.strip().split('-')\n",
    "  if u in userAverage:\n",
    "    predictions.write(u + '-' + i + ',' + str(userAverage[u]) + '\\n')\n",
    "  else:\n",
    "    predictions.write(u + '-' + i + ',' + str(globalAverage) + '\\n')\n",
    "\n",
    "predictions.close()\n",
    "\n",
    "### Would-visit baseline: just rank which businesses are popular and which are not, and return '1' if a business is among the top-ranked\n",
    "\n",
    "businessCount = defaultdict(int)\n",
    "totalPurchases = 0\n",
    "\n",
    "for l in readGz(\"train.json.gz\"):\n",
    "  user,business = l['userID'],l['businessID']\n",
    "  businessCount[business] += 1\n",
    "  totalPurchases += 1\n",
    "\n",
    "mostPopular = [(businessCount[x], x) for x in businessCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "  count += ic\n",
    "  return1.add(i)\n",
    "  if count > totalPurchases/2: break\n",
    "\n",
    "predictions = open(\"predictions_Visit.txt\", 'w')\n",
    "for l in open(\"pairs_Visit.txt\"):\n",
    "  if l.startswith(\"userID\"):\n",
    "    #header\n",
    "    predictions.write(l)\n",
    "    continue\n",
    "  u,i = l.strip().split('-')\n",
    "  if i in return1:\n",
    "    predictions.write(u + '-' + i + \",1\\n\")\n",
    "  else:\n",
    "    predictions.write(u + '-' + i + \",0\\n\")\n",
    "\n",
    "predictions.close()\n",
    "\n",
    "### Category prediction baseline: Just consider some of the most common words from each category\n",
    "\n",
    "catDict = {\n",
    "  \"American Restaurant\": 0,\n",
    "  \"Bar\": 1,\n",
    "  \"Asian Restaurant\": 2,\n",
    "  \"European Restaurant\": 3,\n",
    "  \"Italian Restaurant\": 4,\n",
    "  \"Fast Food Restaurant\": 5,\n",
    "  \"Mexican Restaurant\": 6,\n",
    "  \"Seafood Restaurant\": 7,\n",
    "  \"Coffee Shop\": 8,\n",
    "  \"Sandwich Shop\": 9\n",
    "}\n",
    "\n",
    "predictions = open(\"predictions_Category.txt\", 'w')\n",
    "predictions.write(\"userID-reviewHash,category\\n\")\n",
    "for l in readGz(\"test_Category.json.gz\"):\n",
    "  cat = catDict['American Restaurant'] # If there's no evidence, just choose the most common category in the dataset\n",
    "  words = l['reviewText'].lower()\n",
    "  if 'america' in words:\n",
    "    cat = catDict['American Restaurant']\n",
    "  if 'bar' in words or 'beer' in words:\n",
    "    cat = catDict['Bar']\n",
    "  if 'asia' in words:\n",
    "    cat = catDict['Asian Restaurant']\n",
    "  if 'europe' in words:\n",
    "    cat = catDict['European Restaurant']\n",
    "  if 'italian' in words:\n",
    "    cat = catDict['Italian Restaurant']\n",
    "  if 'fast' in words:\n",
    "    cat = catDict['Fast Food Restaurant']\n",
    "  if 'mexic' in words:\n",
    "    cat = catDict['Mexican Restaurant']\n",
    "  if 'coffee' in words:\n",
    "    cat = catDict['Coffee Shop']\n",
    "  if 'sandwich' in words:\n",
    "    cat = catDict['Sandwich Shop']\n",
    "  predictions.write(l['userID'] + '-' + l['reviewHash'] + \",\" + str(cat) + \"\\n\")\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alluser = list()\n",
    "allbus = set()\n",
    "added = dict()\n",
    "count = 0\n",
    "valid = open('valid1.txt','w')\n",
    "valid2 = open('valid2.txt','w')\n",
    "valid.write('userID-businessID,actual,prediction\\n')\n",
    "valid2.write('userID-businessID,actual,prediction\\n')\n",
    "for l in readGz(\"train.json.gz\"):\n",
    "    user,business = l['userID'],l['businessID']\n",
    "    if user in alluser:\n",
    "        added[user].append(business)\n",
    "    else:\n",
    "        added[user]= list()\n",
    "        added[user].append(business)\n",
    "        alluser.append(user)\n",
    "    allbus.add(business)\n",
    "    if count >= 100000 and count<200000:\n",
    "        valid.write(user + '-' + business +',1'+\"\\n\")\n",
    "    count +=1\n",
    "for i in range(100000):\n",
    "    u = i%len(alluser)\n",
    "    user = alluser[u]\n",
    "    diffbus = allbus.difference(added[user])\n",
    "    if len(diffbus)>0:\n",
    "        newb = diffbus.pop()\n",
    "        added[user].append(newb)\n",
    "        valid2.write(user + '-' + newb +\",0\" +\"\\n\")\n",
    "valid.close()\n",
    "valid2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1:\n",
      "accuracy:  0.503035\n"
     ]
    }
   ],
   "source": [
    "print 'Question 1:'\n",
    "businessCount = defaultdict(int)\n",
    "totalPurchases = 0\n",
    "for l in readGz(\"train.json.gz\"):\n",
    "    user,business = l['userID'],l['businessID']\n",
    "    businessCount[business] += 1\n",
    "    totalPurchases += 1\n",
    "    if totalPurchases >= 100000:\n",
    "        break\n",
    "mostPopular = [(businessCount[x], x) for x in businessCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:    \n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > totalPurchases/2: \n",
    "        break\n",
    "samples = float(0)\n",
    "correct = float(0)\n",
    "for l in open(\"valid1.txt\"):\n",
    "    samples += 1\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        samples -= 1 \n",
    "        continue\n",
    "    u,i = l.strip().split('-')\n",
    "    i = i.split(',')[0]\n",
    "    if i in return1:\n",
    "        correct += 1      \n",
    "for l in open(\"valid2.txt\"):\n",
    "    samples += 1\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        samples -= 1 \n",
    "        continue\n",
    "    u,i = l.strip().split('-')\n",
    "    i = i.split(',')[0]\n",
    "    if not i in return1:\n",
    "        correct += 1\n",
    "print 'accuracy: ', correct/samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 2:\n",
      "accuracy:  0.605045\n",
      "using threhold of the 23rd percentile of populatity has better accuracy\n"
     ]
    }
   ],
   "source": [
    "print 'Question 2:'\n",
    "businessCount = defaultdict(int)\n",
    "totalPurchases = 0\n",
    "for l in readGz(\"train.json.gz\"):\n",
    "    user,business = l['userID'],l['businessID']\n",
    "    businessCount[business] += 1\n",
    "    totalPurchases += 1\n",
    "    if totalPurchases >= 100000:\n",
    "        break\n",
    "mostPopular = [(businessCount[x], x) for x in businessCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:    \n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > totalPurchases*(0.23): \n",
    "        break\n",
    "samples = float(0)\n",
    "correct = float(0)\n",
    "for l in open(\"valid1.txt\"):\n",
    "    samples += 1\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        samples -= 1 \n",
    "        continue\n",
    "    u,i = l.strip().split('-')\n",
    "    i = i.split(',')[0]\n",
    "    if i in return1:\n",
    "        correct += 1      \n",
    "for l in open(\"valid2.txt\"):\n",
    "    samples += 1\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        samples -= 1 \n",
    "        continue\n",
    "    u,i = l.strip().split('-')\n",
    "    i = i.split(',')[0]\n",
    "    if not i in return1:\n",
    "        correct += 1\n",
    "print 'accuracy: ', correct/samples\n",
    "print 'using threhold of the 23rd percentile of populatity has better accuracy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 3:\n",
      "revisit baseline \n"
     ]
    }
   ],
   "source": [
    "print 'Question 3:'\n",
    "print 'revisit baseline '\n",
    "def revisit_baseline(userID,busID,allusercats,allbuscats):\n",
    "    if not userID in allusercats.keys():\n",
    "        return 0\n",
    "    if not busID in allbuscats.keys():\n",
    "        return 0\n",
    "    usercats = allusercats[userID]\n",
    "    buscats = allbuscats[busID]\n",
    "    for bc in buscats:\n",
    "        if bc in usercats:\n",
    "            return 1\n",
    "    return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 4:\n",
      "user name: ah\n",
      "score: 0.66490\n",
      "email: kyhor@ucsd.edu\n"
     ]
    }
   ],
   "source": [
    "print 'Question 4:'\n",
    "allusercats = dict()\n",
    "allbuscats = dict()\n",
    "for l in readGz(\"train.json.gz\"):\n",
    "    user,bus = l['userID'],l['businessID']\n",
    "    if not user in allusercats.keys():\n",
    "        allusercats[user] = set()\n",
    "    if not bus in allbuscats.keys():\n",
    "        allbuscats[bus] = set()\n",
    "    for c in l['categories']:\n",
    "        allusercats[user].add(c)\n",
    "        allbuscats[bus].add(c)\n",
    "predictions = open(\"predictions_revisit.txt\", 'w')\n",
    "for l in open(\"pairs_Visit.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,i = l.strip().split('-')\n",
    "    if revisit_baseline(u,i,allusercats,allbuscats):\n",
    "        predictions.write(u + '-' + i + \",1\\n\")\n",
    "    else:\n",
    "        predictions.write(u + '-' + i + \",0\\n\")\n",
    "predictions.close()\n",
    "print 'user name: ah'\n",
    "print 'score: 0.66490'\n",
    "print 'email: kyhor@ucsd.edu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare data for Question 5:\n"
     ]
    }
   ],
   "source": [
    "print 'prepare data for Question 5:'\n",
    "def mostpop(cidcounts):\n",
    "    mostpop = list()\n",
    "    for i in range(len(cidcount)):\n",
    "        mostpop.append((cidcount[i],i))\n",
    "    mostpop = sorted(mostpop)\n",
    "    return mostpop\n",
    "\n",
    "with_cid = list()\n",
    "for l in readGz('train.json.gz'):\n",
    "    if 'categoryID' in l.keys():\n",
    "        with_cid.append(l)\n",
    "train = with_cid[:len(with_cid)/2]\n",
    "valid = with_cid[len(with_cid)/2:]\n",
    "cidcount = [0,0,0,0,0,0,0,0,0,0] \n",
    "for t in train:\n",
    "    cidcount[int(t['categoryID'])] += 1\n",
    " \n",
    "most_popular = mostpop(cidcount)\n",
    "poprank = dict()\n",
    "for i in range(1,11):\n",
    "    poprank[most_popular[-i][1]] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare data for Question 5:\n"
     ]
    }
   ],
   "source": [
    "print 'prepare data for Question 5:'\n",
    "train_user_cids = dict()\n",
    "predict_visit = dict()\n",
    "for t in train:\n",
    "    u,c = t['userID'],t['categoryID']\n",
    "    if not u in train_user_cids.keys():\n",
    "        train_user_cids[u] = [0,0,0,0,0,0,0,0,0,0]\n",
    "    train_user_cids[u][int(c)] += 1\n",
    "    \n",
    "for u in train_user_cids.keys():\n",
    "    mp = mostpop(train_user_cids[u])\n",
    "    if mp[-1][0] != mp[-2][0]:\n",
    "        predict_visit[u] = mp[-1][1]\n",
    "    else:\n",
    "        if poprank[mp[-1][1]] < poprank[mp[-2][1]]:\n",
    "            predict_visit[u] = mp[-1][1]\n",
    "        else:\n",
    "            predict_visit[u] = mp[-2][1]    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continue Question 5:\n",
      "accuracy:  0.303265143313\n"
     ]
    }
   ],
   "source": [
    "print 'continue Question 5:'\n",
    "samples = float(0)\n",
    "correct = float(0)\n",
    "nosee = 0\n",
    "for t in valid:\n",
    "    samples += 1\n",
    "    u,c  = t['userID'],t['categoryID']\n",
    "    if not u in predict_visit.keys():\n",
    "        predict_cid  = 0\n",
    "        nosee +=1\n",
    "    else:\n",
    "        predict_cid = predict_visit[u]\n",
    "    if int(c) == int(predict_cid):\n",
    "        correct += 1\n",
    "print 'accuracy: ',correct/samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 6:\n",
      "in category  0\n",
      "the 10 most freq words compare to other category:\n",
      "[(0.002175286194542249, u'was'), (0.001500068333356113, u'brunch'), (0.0013399477708666598, u'food'), (0.0011746499194647225, u'breakfast'), (0.0011005589218836542, u'the'), (0.0009329331391129273, u'menu'), (0.0009089905274235052, u'had'), (0.000886761295411112, u'we'), (0.0008312637692096166, u'service'), (0.0007225987727119151, u'cheese')]\n",
      "in category  1\n",
      "the 10 most freq words compare to other category:\n",
      "[(0.00701462999249837, u'a'), (0.006821227082725281, u'bar'), (0.004088571612044616, u'beer'), (0.003733732714768682, u'drinks'), (0.003436296934870655, u'to'), (0.0023448711410517186, u'music'), (0.0022085462291689412, u'drink'), (0.002184483474731238, u'place'), (0.0019377070246698307, u'great'), (0.0019194253737134272, u'on')]\n",
      "in category  2\n",
      "the 10 most freq words compare to other category:\n",
      "[(0.0036505782016691274, u'sushi'), (0.0034279808752587784, u'thai'), (0.00283088925374733, u'food'), (0.0018819230336621314, u'chinese'), (0.0017758869096226333, u'indian'), (0.001714629619503988, u'ramen'), (0.0017007372866780436, u'noodles'), (0.0016597045001939326, u'dishes'), (0.0016414764900811955, u'spicy'), (0.0015997406998564554, u'rice')]\n",
      "in category  3\n",
      "the 10 most freq words compare to other category:\n",
      "[(0.0052544144488884045, u'pizza'), (0.0021033283259192298, u'greek'), (0.002069022802490735, u'tapas'), (0.001884410832389204, u'beer'), (0.0014396067802135688, u'german'), (0.0013528984000063908, u'was'), (0.001312396558186601, u'irish'), (0.0012888843037278871, u'and'), (0.0011370927350144262, u'selection'), (0.0011000415535234297, u'great')]\n",
      "in category  4\n",
      "the 10 most freq words compare to other category:\n",
      "[(0.011207066202888968, u'pizza'), (0.007388416328120984, u'italian'), (0.002710103282768671, u'pasta'), (0.002013858605495693, u'wine'), (0.0018986211818658707, u'pizzas'), (0.0015970016060480857, u'restaurant'), (0.0014074548043126194, u'bread'), (0.0013880640358192326, u'very'), (0.0012647406991659036, u'crust'), (0.0010659039946276283, u'dinner')]\n",
      "in category  5\n",
      "the 10 most freq words compare to other category:\n",
      "[(0.012826947378188602, u'burger'), (0.00855198590504825, u'fries'), (0.007710177214924195, u'burgers'), (0.004921990401718415, u'are'), (0.0034982354038935654, u'fast'), (0.0019950578743474466, u'you'), (0.0018688445656536296, u'they'), (0.0017272377734654483, u'their'), (0.0013301900138296943, u'sandwich'), (0.0013137802400723392, u'get')]\n",
      "in category  6\n",
      "the 10 most freq words compare to other category:\n",
      "[(0.008286398759824433, u'mexican'), (0.006626800245453056, u'tacos'), (0.004181219222944536, u'food'), (0.0033970522649591464, u'taco'), (0.0028444486406160583, u'margaritas'), (0.0028319757010799693, u'salsa'), (0.0027516717130719576, u'burrito'), (0.002137910200811109, u'are'), (0.001664989839715766, u'chips'), (0.0016416075106612475, u'guacamole')]\n",
      "in category  7\n",
      "the 10 most freq words compare to other category:\n",
      "[(0.006272328295358751, u'seafood'), (0.00400446683413696, u'fish'), (0.003477326999261822, u'was'), (0.0033507210670156136, u'we'), (0.003257046337286488, u'lobster'), (0.002696693871776414, u'oysters'), (0.00258580149905419, u'crab'), (0.002007316602284844, u'fresh'), (0.0017662751819412237, u'the'), (0.0016548325849343008, u'chowder')]\n",
      "in category  8\n",
      "the 10 most freq words compare to other category:\n",
      "[(0.0232878688844021, u'coffee'), (0.0029825656746150305, u'shop'), (0.002445295192317125, u'starbucks'), (0.0020598776555306487, u'espresso'), (0.002050067339727523, u'i'), (0.001974491008991987, u'wifi'), (0.001970572956363684, u'cup'), (0.0019066246235643792, u'tea'), (0.0018962920951300852, u'to'), (0.0018394683239147907, u'cafe')]\n",
      "in category  9\n",
      "the 10 most freq words compare to other category:\n",
      "[(0.007877076819206828, u'sandwiches'), (0.0074071285966947325, u'sandwich'), (0.0029028993204293584, u'bread'), (0.002897498333143152, u'cheese'), (0.0028120777134341387, u'their'), (0.002596173878376931, u'deli'), (0.00190123452716358, u'they'), (0.0016866537630584946, u'lunch'), (0.0016192162856795561, u'are'), (0.0013669009474340452, u'beef')]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print 'Question 6:'\n",
    "\n",
    "def feq_500_word_in_cat(cat):\n",
    "    wordCount = defaultdict(int)\n",
    "    punctuation = set(string.punctuation)\n",
    "    for t in train:\n",
    "        if int(t['categoryID']) == int(cat):\n",
    "            r = ''.join([c for c in t['reviewText'].lower() if not c in punctuation])\n",
    "            for w in r.split():\n",
    "                wordCount[w] += 1\n",
    "    counts = [(wordCount[w], w) for w in wordCount]\n",
    "    counts.sort()\n",
    "    counts.reverse()\n",
    "    words500 = [x[1] for x in counts[:500]]\n",
    "    counts500 = [x[0] for x in counts[:500]]\n",
    "    return counts[:500],wordCount\n",
    "\n",
    "def more_feq_in(cat,wordCount,total_count_500):\n",
    "    print 'in category ',cat\n",
    "    morefq = list()\n",
    "    ci,wc = feq_500_word_in_cat(cat)\n",
    "    wi_500 =  [x[1] for x in ci[:500]]\n",
    "    ci_500 = [x[0] for x in ci[:500]]\n",
    "    ci_total500 = sum(ci_500)\n",
    "    for i in range(500):\n",
    "        w = wi_500[i]\n",
    "        c_in_i = wc[w]\n",
    "        fq_in_i = float(c_in_i)/ci_total500\n",
    "        c_w_app = wordCount[w]\n",
    "        fq_w = float(c_w_app)/total_count_500\n",
    "        morefq.append((fq_in_i - fq_w ,w))\n",
    "    morefq.sort()\n",
    "    morefq.reverse()\n",
    "    print 'the 10 most freq words compare to other category:'\n",
    "    print morefq[:10]\n",
    "\n",
    "\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for t in train:\n",
    "    r = ''.join([c for c in t['reviewText'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        wordCount[w] += 1\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "words500 = [x[1] for x in counts[:500]]\n",
    "counts500 = [x[0] for x in counts[:500]]\n",
    "total_count_500 = sum(counts500)\n",
    "\n",
    "for i in range(10):\n",
    "    more_feq_in(i,wordCount,total_count_500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 7:\n",
      "data is ready\n",
      "c =  0.01  with accuracy:  0.0\n",
      "c =  0.1  with accuracy:  0.0\n",
      "c =  1  with accuracy:  2.8491651946e-05\n",
      "c =  10  with accuracy:  0.0359849564078\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-5da9da3b83d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[0mvalid_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpre\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Khor_000\\Anaconda2\\lib\\site-packages\\sklearn\\svm\\base.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m         \u001b[1;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Khor_000\\Anaconda2\\lib\\site-packages\\sklearn\\svm\\base.pyc\u001b[0m in \u001b[0;36m_dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    252\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\svm\\libsvm.pyx\u001b[0m in \u001b[0;36msklearn.svm.libsvm.fit\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "print 'Question 7:'\n",
    "\n",
    "def featQ7 (t,word500Pos):\n",
    "    feat = [0]*500\n",
    "    punctuation = set(string.punctuation)\n",
    "    r = ''.join([c for c in t['reviewText'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        feat[word500Pos[w]] = 1\n",
    "    return feat\n",
    "\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for t in train:\n",
    "    r = ''.join([c for c in t['reviewText'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        wordCount[w] += 1\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "words500 = [x[1] for x in counts[:500]]\n",
    "word500Pos = defaultdict(int)\n",
    "for i in range(len(words500)):\n",
    "    word500Pos[words500[i]] = i\n",
    "X_train = [featQ7(t,word500Pos) for t in train]\n",
    "X_valid = [featQ7(t,word500Pos) for t in valid]\n",
    "y_train = [ 1 == int(t['categoryID'])for t in train]\n",
    "y_valid = [ int(t['categoryID']) for t in valid]\n",
    "bestc = -1\n",
    "bestacc = -1\n",
    "print 'data is ready'\n",
    "for c in [0.01,0.1,1,10,100]:    \n",
    "    clf = svm.SVC(C=c)\n",
    "    clf.fit(X_train, y_train)\n",
    "    valid_pred = clf.predict(X_valid)\n",
    "    acc = [(pre and val) for (pre,val) in zip(valid_pred,y_valid)]\n",
    "    correct = sum (acc)\n",
    "    accuracy = float(correct)/len(acc)\n",
    "    print 'c = ',c,' with accuracy: ', accuracy\n",
    "    if accuracy > bestacc:\n",
    "        bestacc = accuracy\n",
    "        bestc = c\n",
    "print 'c = ',bestc,' has highest accuracy:',bestacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 8:\n"
     ]
    }
   ],
   "source": [
    "print 'Question 8:'\n",
    "\n",
    "def featQ8(t,word500Pos):\n",
    "    feat = [0]*500\n",
    "    punctuation = set(string.punctuation)\n",
    "    r = ''.join([c for c in t['reviewText'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        feat[word500Pos[w]] = 1\n",
    "    return feat\n",
    "\n",
    "def train_catsvm(cat,train,c,X_train):\n",
    "    y_train = [int(t['categoryID']) == int(cat) for t in train]\n",
    "    clf = svm.SVC(C=c)\n",
    "    clf.fit(X_train,y_train)\n",
    "    return clf\n",
    "\n",
    "def find_best_c_for_catsvm(tarin,valid,cat,X_train,X_valid):\n",
    "    y_valid = [int(cat) == int(t['categoryID'])for t in valid]\n",
    "     #y_valid = [ int(t['categoryID'])for t in valid]\n",
    "    bestc = -1\n",
    "    bestacc = -1\n",
    "    for c in [0.01,0.1,1,10,100]:    \n",
    "        clf = train_catsvm(cat,train,c,X_train)\n",
    "        valid_pred = clf.predict(X_valid)\n",
    "        acc = [(pre == val) for (pre,val) in zip(valid_pred,y_valid)]\n",
    "        correct = sum (acc)\n",
    "        accuracy = float(correct)/len(acc)\n",
    "        if accuracy > bestacc:\n",
    "            bestacc = accuracy\n",
    "            bestc = c\n",
    "            \n",
    "    return bestc\n",
    "\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for t in train:\n",
    "    r = ''.join([c for c in t['reviewText'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        wordCount[w] += 1\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "word500 = [x[1] for x in counts[:500]]\n",
    "word500Pos = defaultdict(int)\n",
    "for i in range(len(words500)):\n",
    "    word500Pos[words500[i]] = i\n",
    "X_train = [featQ8(t,word500Pos) for t in train]\n",
    "X_valid = [featQ8(t,word500Pos) for t in valid]\n",
    "y_valid_cid = [int(rev['categoryID']) for rev in valid]\n",
    "best_c = [find_best_c_for_catsvm(train,valid,cat,X_train,X_valid) for cat in range(10)]\n",
    "best_catsvm = [train_catsvm(cat,train,best_c[cat],X_train)for cat in range(10)]\n",
    "print 'Data ready'\n",
    "correct = flaot(0)\n",
    "samples = len(y_valid_cid) \n",
    "for i in len(y_valid):\n",
    "    best_score = -1\n",
    "    best_guess = -1\n",
    "    for cat in range(10):\n",
    "        clf = best_catsvm[cat]\n",
    "        score = clf.decision_function(X_valid[i])\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_guess = cat\n",
    "    if y_valid_cid[i] == best_guess:\n",
    "        correct += 1\n",
    "print 'Accuracy: ',correct/samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
